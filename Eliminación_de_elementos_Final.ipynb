{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Librerías"
      ],
      "metadata": {
        "id": "nAGM_uLNrdcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download es_core_news_sm\n",
        "!pip install pdfplumber --no-cache-dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bywLsLq8KGoD",
        "outputId": "fbc52d58-3fc9-4ba6-a84b-951acc16150f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting es-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.7)\n",
            "Requirement already satisfied: pdfminer.six==20250506 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20250506)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.3.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.3)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "VX_xx95ZIg7Z"
      },
      "outputs": [],
      "source": [
        "import pdfplumber  # Para extraer texto de PDFs\n",
        "import re  # Para trabajar con expresiones regulares\n",
        "import spacy  # Para procesamiento de lenguaje natural (NLP)\n",
        "import unicodedata  # Para normalizar caracteres Unicode\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lectura y extracción de la información"
      ],
      "metadata": {
        "id": "mT6aaDv07Rm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/certificado.pdf\""
      ],
      "metadata": {
        "id": "9-7DSywc7YmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " Importar la librería para trabajar con expresiones regulares\n",
        "import re\n",
        "\n",
        "header_pattern = r\"Cámara de Comercio de Barranquilla\\s*CERTIFICADO DE EXISTENCIA Y REPRESENTACION LEGAL O\\s*DE INSCRIPCION DE DOCUMENTOS\\.\\s*Fecha de expedición:.*?\\nRecibo No\\..*?\\nCODIGO DE VERIFICACIÓN:.*?\\n\""
      ],
      "metadata": {
        "id": "g77Kfy0O7Z6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_without_header(pdf_path):\n",
        "      extracted_text = []  # Lista donde almacenaremos el texto de cada página\n",
        "\n",
        "      with pdfplumber.open(pdf_path) as pdf:  # Abrir el archivo PDF\n",
        "          for page in pdf.pages:  # Iterar por cada página del PDF\n",
        "              text = page.extract_text()  # Extraer texto de la página\n",
        "              if text:  # Verificar si hay texto en la página\n",
        "                  text = re.sub(header_pattern, \"\", text, flags=re.DOTALL)  # Eliminar encabezado con regex\n",
        "                  extracted_text.append(text)  # Guardar el texto limpio en la lista\n",
        "\n",
        "      return \" \".join(extracted_text)  # Unir todas las páginas en un solo texto"
      ],
      "metadata": {
        "id": "dTiShRQd7lHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NKLT"
      ],
      "metadata": {
        "id": "IR26LzV6rhO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener las stopwords predeterminadas de NLTK en español\n",
        "nltk_stopwords = set(stopwords.words('spanish'))\n",
        "\n",
        "# Define las stopwords que quiero conservar\n",
        "stopwords_to_keep = {\"sin\", \"no\", \"contra\"}\n",
        "\n",
        "# Crear un conjunto final de stopwords a eliminar\n",
        "final_stopwords = nltk_stopwords - stopwords_to_keep\n",
        "\n",
        "# --- FUNCIONES DEL PROGRAMA ---\n",
        "\n",
        "# Limpieza y preparación del texto\n",
        "\n",
        "def preprocess_text_nltk(text):\n",
        "    \"\"\"Preprocesa el texto usando NLTK, conservando números y stopwords específicas.\"\"\"\n",
        "    # Normalizar caracteres Unicode y convertir a minúsculas\n",
        "    text = unicodedata.normalize(\"NFKD\", text).lower()\n",
        "\n",
        "    # Tokeniza el texto using NLTK, removed language='spanish'\n",
        "    tokens_nltk = word_tokenize(text)\n",
        "\n",
        "    # Filtra los tokens: mantiene solo letras alfabéticas y números, y elimina las stopwords menos las stopwords_to_keep\n",
        "    final_tokens = [\n",
        "        token for token in tokens_nltk\n",
        "        if (token.isalpha() or token.isdigit()) and token not in final_stopwords\n",
        "    ]\n",
        "    return final_tokens\n",
        "\n",
        "#\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Extraer el texto limpio del PDF y eliminar el encabezado\n",
        "    raw_text = extract_text_without_header(pdf_path)\n",
        "\n",
        "    if raw_text:\n",
        "        # 2. Preprocesar el texto con la función de NLTK\n",
        "        tokens = preprocess_text_nltk(raw_text)\n",
        "\n",
        "        # 3. Mostrar los primeros 50 tokens como prueba\n",
        "        print(\"Primeros 50 tokens preprocesados:\")\n",
        "        print(tokens[:50])\n",
        "\n",
        "        # 4. Guardar el resultado en un nuevo archivo de texto\n",
        "        output_file_path = \"/content/certificado_limpio.txt\"\n",
        "        with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\" \".join(tokens))\n",
        "        print(f\"\\nTexto preprocesado guardado en: {output_file_path}\")\n",
        "    else:\n",
        "        print(\"No se pudo procesar el PDF o no se extrajo texto.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iii2A9au4s_0",
        "outputId": "36d4a1fb-6c91-4ced-d96a-a139a81a5bb4"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primeros 50 tokens preprocesados:\n",
            "['verifique', 'contenido', 'confiabilidad', 'certificado', 'ingresando', 'digite', 'visualice', 'imagen', 'generada', 'momento', 'puede', 'realizar', 'manera', 'ilimitada', '60', 'calendario', 'contados', 'partir', 'fecha', 'atencion', 'comerciante', 'no', 'cumplido', 'deber', 'legal', 'renovar', 'matricula', 'mercantil', 'd', 's', 'matricula', 'mercantil', 'proporciona', 'seguridad', 'confianza', 'ean', 'n', 'renueve', 'matricula', 'mercantil', 'mas', 'tardar', '31', 'marzo', 'v', 'i', 'c', 'n', 'n', 's']\n",
            "\n",
            "Texto preprocesado guardado en: /content/certificado_limpio.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SpaCy"
      ],
      "metadata": {
        "id": "IRQshncO6llP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"es_core_news_sm\")  #cargar el modelo en español"
      ],
      "metadata": {
        "id": "o2uH2cLx7eps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # STOPWORDS\n",
        "  # NÚMEROS\n",
        "\n",
        "  # Obtener todas las stopwords predeterminadas de spaCy\n",
        "  spacy_stopwords = nlp.Defaults.stop_words\n",
        "\n",
        "  # Define las stopwords que quiero conservar\n",
        "  stopwords_to_keep = {\"sin\", \"no\", \"contra\"}\n",
        "\n",
        "  # Crear un conjunto final de stopwords a eliminar\n",
        "  final_stopwords = spacy_stopwords - stopwords_to_keep\n",
        "\n",
        "  def preprocess_text(text):\n",
        "      text = unicodedata.normalize(\"NFKD\", text)\n",
        "      doc = nlp(text.lower())\n",
        "\n",
        "      # Se filtran los tokens en aquellos que son solo alfabéticos o solo números y que no están en la lista final de stopwords\n",
        "      tokens = [token.text for token in doc if (token.is_alpha or token.is_digit ) and token.text not in final_stopwords]\n",
        "      return tokens"
      ],
      "metadata": {
        "id": "N87p5a1hvi6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = extract_text_without_header(pdf_path)  # Extraer texto limpio del PDF\n",
        "tokens = preprocess_text(raw_text)  # Preprocesar el texto con spaCy\n",
        "\n",
        "print(tokens[:50])  # Mostrar los primeros 50 tokens como prueba"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJGPMe_KK9pZ",
        "outputId": "6841912d-e429-4f06-9109-6b9946862d1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['verifique', 'contenido', 'confiabilidad', 'certificado', 'ingresando', 'digite', 'visualice', 'imagen', 'generada', 'momento', 'ilimitada', 'calendario', 'contados', 'fecha', 'atencion', 'comerciante', 'cumplido', 'deber', 'legal', 'renovar', 'matricula', 'mercantil', 'd', 's', 'matricula', 'mercantil', 'proporciona', 'seguridad', 'confianza', 'laos', 'negocios', 'n', 'renueve', 'matricula', 'mercantil', 'tardar', 'marzo', 'v', 'i', 'c', 'n', 'n', 's', 'fundamento', 'inscripcir', 'ones', 'efectuad', 'as', 'registro', 'mercantil']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/certificado_limpio.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\" \".join(tokens))"
      ],
      "metadata": {
        "id": "fCi7wKkbLErR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}